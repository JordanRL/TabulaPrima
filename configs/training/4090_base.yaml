# Define which optimizer/scheduler configs to use by default
defaults:
  - _self_
  - optimizer: adamw        # Corresponds to configs/optimizer/adamw.yaml
  - scheduler: cosine_token # Corresponds to configs/scheduler/cosine_token.yaml
  - wandb: default

# @package training
batch_size: 4
grad_steps: 8
dropout: 0.0
learning_rate: 5e-5
weight_decay: 0.01
max_grad_norm: 1.0
log_interval: 10
eval_interval: 100
use_amp: false
allow_amp_switchover: true
use_checkpointing: false
use_fusions: false
compile_model: true
target_tokens_per_param: 10

# Checkpointing/Output settings
checkpoint_dir: "checkpoints"
model_dir: "models"