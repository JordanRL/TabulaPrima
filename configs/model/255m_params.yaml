defaults:
  - model_schema

# @package _group_
_target_: model_arch.MLATransformer

# Parameters matching MLATransformer.__init__
hidden_dim: 1152
num_layers: 12
num_heads: 9
ff_dim: 4608
kv_latent_dim: 288
q_latent_dim: 288
dropout: ${training.dropout}
max_seq_len: ${dataset.seq_length}
rope_head_dim: 32
use_checkpointing: ${training.use_checkpointing}
use_fusions: ${training.use_fusions}