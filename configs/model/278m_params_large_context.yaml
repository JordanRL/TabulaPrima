defaults:
  - _self_

# @package model
_target_: model_arch.MLATransformer
hidden_dim: 1152
num_layers: 12
num_heads: 9
ff_dim: 4608
kv_latent_dim: 288
q_latent_dim: 288
dropout: ${training.dropout}
max_seq_len: 1024
rope_head_dim: 32
use_checkpointing: ${training.use_checkpointing}
use_fusion: ${training.use_fusions}